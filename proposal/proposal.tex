\documentclass[11pt,a4paper]{article}

\usepackage{float}
\usepackage{hyperref}

\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\lstset{
	language=Java,
	basicstyle=\small\ttfamily,
	keywordstyle=\color{Blue}\bf,
	tabsize=2,
	frame=single
}

\title{HIVE-13680: Provide a way to compress ResultSets}
\author{Kevin Liew}

\begin{document}

\maketitle

\begin{abstract}
	Hive data pipelines invariably involve JDBC/ODBC drivers which access Hive through its Thrift interface. 
	Consequently, any enhancement to the Hive's Thrift data pipeline will benefit all users.
	
	Prior to 
	\href{https://issues.apache.org/jira/browse/HIVE-12049}{HIVE-12049}
	, HiveServer2 would read a full ResultSet from HDFS before deserializing and re-serializing into Thrift objects for RPC transfer.
	Following our enhancement, task nodes serialize Thrift objects from their own block of a ResultSet file. 
	HiveServer2 reads the Thrift output and deserializes all columns as one binary blob rather than column-by-column and transfers the ResultSet to the remote client. 
	This parallel serialization strategy reduced latency in the data pipeline.
	
	However, network capacity is often the most scarce resource in a system. 
	As a further enhancement, network load can be eased by having task nodes compress their own block of a ResultSet as part of the serialization process.
\end{abstract}

\section{Introduction}
	The changes proposed herein draw from Rohit Dholakia's design document and patches for
	\href{https://issues.apache.org/jira/browse/HIVE-10438}{HIVE-10438}, which implemented compression on HiveServer2.
	Now that
	\href{https://issues.apache.org/jira/browse/HIVE-12049}{HIVE-12049}
	has been committed, compression can take place in parallel on the task nodes.
	
	Our goals for this enhancement are to:
	\begin{itemize}
		\item improve performance out-of-the-box for new clients
		\item maintain compatibility with old clients
		\item provide flexibility yet security
		\item confer a simple interface
	\end{itemize}
	
\section{Design Overview}

	\subsection{Compressor Interface}
		We will define a compressor interface which must be implemented by compressor plugins.
		A Snappy compressor will compress all data-types using the Snappy algorithm.
		Type-specific compression can be achieved by implementing a compressor that delegates compression to other compressors based on a case-switch block.
		
	\subsection{Client-Server Negotiation}
		The client will specify a preferred compressor (and crc for the plugin's jar file) upon connection. 
		If the server does not have that compressor plugin, it will request the jar from the client. 
		The client should send the compressor plugin and all dependencies.
		
		To maintain compatibility with old clients, the server will not compress results unless the client has requested compression.
		However, new versions of beeline will try to negotiate a compressor scheme by default.
		
	\subsection{Server Security}
		We place the onus of security on the server administrator, recommending Kerberos authentication with user-impersonation to limit the jar file to the same permissions as the authenticated user. 
		The server administrator also has the option to disable compressor plugins altogether.
		
	\subsection{Configuration options}
		\begin{table}[H]
			\begin{tabular}{| p{3cm} | c | p{6.5cm} |} \hline
				\textbf{Option} & \textbf{Default} & \textbf{Description} \\ \hline
					hive.resultSet\linebreak
					.compressor\linebreak
					.enabled
				 & true & Enable or disable compressor negotiation \\ \hline
					 hive.server2\linebreak
					 .thrift\linebreak
					 .resultset.max\linebreak
					 .fetch.size
				 & 1000 & Max number of rows sent in one Fetch RPC call by the server to the client. Also max number of rows in one compressed batch.
				 \\ \hline
			\end{tabular}
			\caption{Configuration options}
		\end{table}

\section{Implementation}

	\subsection{Negotiation}
		TBD
		
	\subsection{Compression}
		The output of a final node in a DAG is either a set of rows (from a map task) or a single value (from a reduce task).
		Following
		\href{https://issues.apache.org/jira/browse/HIVE-12049}{HIVE-12049}
		, the output is buffered row-by-row into TColumns in TRowSet and serialized in batches to a file in HDFS.
		The final output file is read by HiveServer2 as a binary blob and sent to the client.
		
		Compressors will operate on the batch-level in ThriftJDBCBinarySerDe and compressed batches will be serialized contiguously in the output file. 
		
		In the event that a column is not compressible, the column will be serialized as an uncompressed column.
		
	\subsection{Decompression}
		Results are serialized in contiguous compressed batches. Consequently, the client must deserialize batch-by-batch.
		
		TBD
		%where does decompression occur in the client? will we operate on (Encoded)ColumnBasedSet or directly on TRowSet?
		%The client will have the entire blob containing contiguous batches, or does it receive batches separately? 
		
	\subsection{Compressed RowSet Structures}
		\subsubsection{HiveServer2 Structures}
			TBD
		
		\subsubsection{Thrift Objects}
			TEnColumn must now store a list of batch offsets to delimit the compressed blob and allow decompression of contiguous batches of rows within each column. TRowSet will have a list of TEnColumn to store compressed columns.
			Otherwise, the Thrift structures used to support compression are largely unchanged from 
			\href{https://issues.apache.org/jira/browse/HIVE-10438}{HIVE-10438}
			. They are described here for completeness.
			
			\begin{lstlisting}[title=TCLIService.thrift,gobble=8,otherkeywords={binary,i32,i64,string,struct,TColumn,TEnColumn,TRow,TRowSet,TTypeId}]
				//Represents an encoded column
				struct TEnColumn {
					1: required binary enData
					2: required binary nulls
					3: required TTypeId type
					4: required list<i32> batchSize
				}
				
				// Represents a rowset
				struct TRowSet {
					// The starting row offset of this rowset.
					1: required i64 startRowOffset
					2: required list<TRow> rows
					3: optional list<TColumn> columns
					4: optional binary binaryColumns
					5: optional i32 columnCount
					6: optional list<TEnColumn> enColumns
				}
			\end{lstlisting}
			
			TEnColumns `enData` is a binary blob containing the contiguous compressed batches within the node's block of the output file.
			`nulls` is a bitmap indicating null rows in the column set.
			`type` is the column's data-type.
			`batchSize` contains the batch size for each compressed batch.
			
			TRowSet `startRowOffet` and `rows` are deprecated following \href{https://issues.apache.org/jira/browse/HIVE-3746}{HIVE-3746}.
			Result files are now column-oriented and either in `binaryColumns` or a combination of `columns` and `enColumns`.
			
	\subsection{Compressor-Decompressor Interface}
		\begin{lstlisting}[title=org.apache.hive.service.cli.CompDe; CompDe.java,gobble=6,otherkeywords={ColumnBuffer}]
			@InterfaceAudience.Private
			@InterfaceStability.Stable
			public interface ColumnCompDe {
				public byte[] compress(ColumnBuffer columns);
				public ColumnBuffer decompress(
					byte[] columnBlob, int startByte, int endByte);
				public boolean isCompressible(ColumnBuffer columns);
			}
		\end{lstlisting}
		
		Operating in the final task node, `compress` takes a batch of rows contained in `columns` (number of rows will be equal to or less than \linebreak hive.server2.thrift.resultset.max.fetch.size) and outputs a binary blob.
		The compressor is free to pack additional details such as look-up tables within this blob.
		
		TBD
		%The client will have the entire blob containing contiguous batches, or does it receive batches separately? 
			
\section{Custom Compressors}
		A default compressor will be provided for Snappy compression.
		To use other compression algorithms or to have type-specific compression, the user must implement the CompDe interface.
		
		TBD
		
\end{document}
